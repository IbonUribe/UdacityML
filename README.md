# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary

The dataset Bank Marketing (bankmarketing_train.csv) contains anonymized data over people (32,950) who suscribed to a fixed deposit, with variables such age, job title, marital status, education level, housing, loan, type of contact...and whether they finally suscribed it or not. With this data and a right prediction model we could be able to predict if a person could be a reasonable lead and become a customer of the offered service in a marketing campaign.

In this project, we are exploring two different ways of deploying an optimized Machine Learning (ML) pipeline in order to build that prediction model. A method in which a machine learning engineer must provide almost all the code except for hyperparameter tuning that could be accomplished using Azure HyperDrive package, and a method in which even this adjustment is done automatically along with the rest of the pipeline using Azure AutoML.

The best performing model with no meaningful differences against the custom model, has been the one generated by AutoML, with an accuracy of 91.64% Custom model has scored a bit lower with a 91.41% of accuracy. In the model generated by AutoML, the resulting algorithm has been Voting Ensemble that scored better than Logistic Regression model from SciKit-Learn, with a strength (--C) of 100 and 50 as max number of iterations (--max_iter).

## Common stages from both Pipelines
Both pipelines follow the classic ETL approach of Extracting data, Transforming data to "clean" data and Loading data into the machine learning algorithm, and in this exploration, both pipelines do use similar code to fulfill those stages.

1. Extracting data.

There is just one data source and it comes in a file with fields separated by commas, so providing it the file path a TabularDataset is created. As stated before, the dataset contains 32,950 rows and each row is related to a person and its different features captured during a Bank marketing campaign.

2. Transforming data.

Function to "clean" data makes use of Panda's library in a process that basically transforms string type values into binary data; one hot encoding.

3. Loading data.

Splits data into train and test sets. Train set consists on the 80% of the whole set and is used so the  ML algorithm can "learn" and the other 20% that belongs to test data is used to check whether the model is well fitted.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
In this case, a linear logistic regression classification algorithm from sklearn-scikit package is used in order to try fitting to the nature of the data aiming to have a model that could make an accurate prediction. Two parameters are provided as hyperparameters, strength (--C) and max number of iterations (--max_iter). The strength refers to the regularization of data that algorithm applies, the lower the value, the stronger becomes the regularization. Regularization is a technique used to reduce the "absorption of noise" by the model and avoiding overfitting, one of the main problems of machine learning. Max number of iterations sets the number of iterations taken for the selected algorithm to converge with the sought goal, 100% accuracy, in this case. Even though logistic regression technique is used, the problem to solve is a classification problem, with labeled data (supervised learning) that relates each sample with its class. We need to predict if a person would like to subscribe to a bank deposit or not. In a regression problem we would need to guess a number, not a class (e.g. houses cost in a region).

There are other algorithms to use such KNN Decision tree, Random Forrest or Gradient boosting that may fit better in other contexts if we follow the cheat-sheet provided by Scikit-Learn (https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html). To decide whether an algorithm works or not, we have to consider it's accuracy in relation with the nature of the problema we are trying to solve. Almost 92% of accuracy might be enough to say the chosen algorithm works for our problem.

**What are the benefits of the parameter sampler you chose?**
Out of the three parameter sampling AzureML library offers, RandomParameterSampling is the cheaper, and taking into account the context were we had a credit to use computational resources resulted the best choice. GridParameterSampling is the most exhaustive, and BayesianParameterSampling requires more number of runs, because it picks parameters based on how previous runs performed. Another feature of Bayesian sampling is that early termination policy can not be adopted. For our purpose, RandomParameterSampling resulted convenient.

![Linear Regression Parameters](https://user-images.githubusercontent.com/69908503/120907349-e66d1280-c660-11eb-9001-75b5758bc15e.png)

**What are the benefits of the early stopping policy you chose?**
The benefits of using Bandit policy for an early termination is that it is a bit more aggressive compared to Median Stopping policy, specially for that narrow range we have provided (slack factor of 0.1). The lack of computational resources haven't let me try different factors, but probably a wider band (a higher slack factor) could offer better results, . This is exactly the kind of scenarios where this policy, or Truncation policy, fit better to.

## AutoML
**The model and hyperparameters generated by AutoML.**
In the case of AutoML model, data comes without splitting it into train and test sets, so third stage of ETL is conducted automatically by AutoML depending on the value of  the parameter n_cross_validation. Two different executions with 2 and 4 folds, gave slight different results and a better accuracy with the last value (0.91544 vs. 0.91642).

In both cases VotingEnsemble performed better, using LightGBM, XGBoostClassifier (twice), LogisticRegression, GradientBoosting and RandomForest (twice) algorithms ensemble with these weights 0.2222222222 0.2222222222 0.111111111111 0.111111111111 0.111111111111 0.2222222222

These are used algorithms hyperparameters:

[('1', Pipeline(memory=None,
         steps=[('maxabsscaler', MaxAbsScaler(copy=True)),
                ('xgboostclassifier',
                 XGBoostClassifier(n_jobs=1, problem_info=ProblemInfo(
                                dataset_samples=32950,
                                dataset_features=132,
                                dataset_classes=2,
                                dataset_num_categorical=0,
                                dataset_categoricals=None,
                                pipeline_categoricals=None,
                                dataset_y_std=None,
                                dataset_uid=None,
                                subsampling=False,
                                task='classifi...
                                subsampling_schedule='hyperband_clip',
                                cost_mode_param=None,
                                iteration_timeout_mode=0,
                                iteration_timeout_param=None,
                                feature_column_names=None,
                                label_column_name=None,
                                weight_column_name=None,
                                cv_split_column_names=None,
                                enable_streaming=None,
                                timeseries_param_dict=None,
                                gpu_training_param_dict={'processing_unit_type': 'cpu'}
                            ), random_state=0, tree_method='auto'))],
         verbose=False)), ('6', Pipeline(memory=None,
         steps=[('sparsenormalizer', Normalizer(copy=True, norm='l2')),
                ('xgboostclassifier',
                 XGBoostClassifier(booster='gbtree', colsample_bytree=0.9, eta=0.3, gamma=0, max_depth=9, max_leaves=0, n_estimators=25, n_jobs=1, objective='reg:logistic', problem_info=ProblemInfo(
                                dataset_samples=32950,
                                dataset_features=132,
                                dataset_classes=2,
                                dataset_num_cat...
                                iteration_timeout_mode=0,
                                iteration_timeout_param=None,
                                feature_column_names=None,
                                label_column_name=None,
                                weight_column_name=None,
                                cv_split_column_names=None,
                                enable_streaming=None,
                                timeseries_param_dict=None,
                                gpu_training_param_dict={'processing_unit_type': 'cpu'}
                            ), random_state=0, reg_alpha=0, reg_lambda=0.7291666666666667, subsample=0.9, tree_method='auto'))],
         verbose=False)), ('0', Pipeline(memory=None,
         steps=[('maxabsscaler', MaxAbsScaler(copy=True)),
                ('lightgbmclassifier',
                 LightGBMClassifier(min_data_in_leaf=20, n_jobs=1, problem_info=ProblemInfo(
                                dataset_samples=32950,
                                dataset_features=132,
                                dataset_classes=2,
                                dataset_num_categorical=0,
                                dataset_categoricals=None,
                                pipeline_categoricals=None,
                                dataset_y_std=None,
                                dataset_uid=None,
                                subsamplin...
                                subsampling_schedule='hyperband_clip',
                                cost_mode_param=None,
                                iteration_timeout_mode=0,
                                iteration_timeout_param=None,
                                feature_column_names=None,
                                label_column_name=None,
                                weight_column_name=None,
                                cv_split_column_names=None,
                                enable_streaming=None,
                                timeseries_param_dict=None,
                                gpu_training_param_dict={'processing_unit_type': 'cpu'}
                            ), random_state=None))],
         verbose=False)), ('9', Pipeline(memory=None,
         steps=[('maxabsscaler', MaxAbsScaler(copy=True)),
                ('logisticregression',
                 LogisticRegression(C=2.559547922699533, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1, l1_ratio=None,
                                    max_iter=100, multi_class='ovr', n_jobs=1,
                                    penalty='l2', random_state=None,
                                    solver='saga', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)), ('7', Pipeline(memory=None,
         steps=[('maxabsscaler', MaxAbsScaler(copy=True)),
                ('gradientboostingclassifier',
                 GradientBoostingClassifier(ccp_alpha=0.0,
                                            criterion='friedman_mse', init=None,
                                            learning_rate=0.046415888336127774,
                                            loss='deviance', max_depth=4,
                                            max_features=0.4,
                                            max_leaf_nodes=None,
                                            min_impurity_decrease=0.0,
                                            min_impurity_split=None,
                                            min_samples_leaf=0.08736842105263157,
                                            min_samples_split=0.5252631578947369,
                                            min_weight_fraction_leaf=0.0,
                                            n_estimators=100,
                                            n_iter_no_change=None,
                                            presort='deprecated',
                                            random_state=None, subsample=1,
                                            tol=0.0001, validation_fraction=0.1,
                                            verbose=0, warm_start=False))],
         verbose=False)), ('4', Pipeline(memory=None,
         steps=[('maxabsscaler', MaxAbsScaler(copy=True)),
                ('randomforestclassifier',
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight='balanced',
                                        criterion='gini', max_depth=None,
                                        max_features='log2',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_impurity_split=None,
                                        min_samples_leaf=0.01,
                                        min_samples_split=0.01,
                                        min_weight_fraction_leaf=0.0,
                                        n_estimators=25, n_jobs=1,
                                        oob_score=True, random_state=None,
                                        verbose=0, warm_start=False))],
         verbose=False)), ('5', Pipeline(memory=None,
         steps=[('maxabsscaler', MaxAbsScaler(copy=True)),
                ('randomforestclassifier',
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight='balanced',
                                        criterion='entropy', max_depth=None,
                                        max_features='sqrt',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_impurity_split=None,
                                        min_samples_leaf=0.01,
                                        min_samples_split=0.2442105263157895,
                                        min_weight_fraction_leaf=0.0,
                                        n_estimators=10, n_jobs=1,
                                        oob_score=False, random_state=None,
                                        verbose=0, warm_start=False))],
         verbose=False))]

Voting Ensemble, as it name recalls, uses a combination of different estimators instead of using one single algorithm. It weights each algorithm resulting class probability, and based on it makes the prediction. To do the selection of algorithms that would form the ensemble, it uses Caruana ensembles selection algorithm that in each ensemble iteration  makes the selection.

![Best performance AutoML](https://user-images.githubusercontent.com/69908503/120907270-72326f00-c660-11eb-8de0-a7828a2ccfaa.png)

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
In this case, the results were almost the same, with a slight difference in favor of AutoML. Given the accuracy of Scikit-Learn pipeline, it doesn't seem to be overfitting. In fact, it could be happening just the opposite, that the used model is not powerful enough to fit better. So in this scenario, selecting the right solver seems to be key to improve performance. The results from AutoML pipeline go much in the same way, were Voting Ensemble performed the best, telling us a combination of different solvers is better than opting for just one of them, and yet, the accuracy is a little bit better than the other pipeline.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
The first area of improvement is going further with the kind of algorithms to use and employ neural nets to get higher precision for this kind of data and needs. Another clear area of improvement is getting a more balanced data, those who made a deposit represented almost one tenth of the total. Other improvement areas are different combinations of parameter sampling, such Grid Parameter Sampling that performs a whole parameter space searching to find the optimal tuning. Of course, trying fitting different classification solvers could help improving accuracy. In fact, AutoML has chosen Voting Ensemble as the algorithm with highest accuracy, note that this algorithm uses a mix of techniques denoting that might not exist a unique approach to improve learning given training data. So logistic regression model might not be the best. As SKLearn estimator is deprecated, moving on to use ScriptRunConfig seems to be a reasonable improvement too.

![Class balancing detection](https://user-images.githubusercontent.com/69908503/120907281-85ddd580-c660-11eb-81c5-76449173e7a0.png)
