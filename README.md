# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary

The dataset Bank Marketing (bankmarketing_train.csv) contains anonymized data over people (32,950) who suscribed to a fixed deposit, with variables such age, job title, marital status, education level, housing, loan, type of contact...and whether they finally suscribed it or not. With this data and a right prediction model we could be able to predict if a person could be a reasonable lead and become a customer of the offered service in a marketing campaign.

In this project, we are exploring two different ways of deploying an optimized Machine Learning (ML) pipeline in order to build that prediction model. A method in which a machine learning engineer must provide almost all the code except for hyperparameter tuning that could be accomplished using Azure HyperDrive package, and a method in which even this adjustment is done automatically along with the rest of the pipeline using Azure AutoML.

The best performing model with no meaningful differences against the custom model, has been the one generated by AutoML, with an accuracy of 91.55% Custom model has scored a bit lower with a 91.41% of accuracy. In the model generated by AutoML, the resulting algorithm has been Voting Ensemble that scored better than Logistic Regression model from SciKit-Learn, with a strength (--C) of 100 and 50 as max number of iterations (--max_iter).

## Common stages from both Pipelines
Both pipelines follow the classic ETL approach of Extracting data, Transforming data to "clean" data and Loading data into the machine learning algorithm, and in this exploration, both pipelines do use similar code to fulfill those stages.

1. Extracting data.

There is just one data source and it comes in a file with fields separated by commas, so providing it the file path a TabularDataset is created. As stated before, the dataset contains 32,950 rows and each row is related to a person and its different features captured during a Bank marketing campaign.

2. Transforming data.

Function to "clean" data makes use of Panda's library in a process that basically transforms string type values into binary data; one hot encoding.

3. Loading data.

Splits data into train and test sets. Train set consists on the 80% of the whole set and is used so the  ML algorithm can "learn" and the other 20% that belongs to test data is used to check whether the model is well fitted.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
n this case, a linear logistic regression classification algorithm from sklearn-scikit package is used in order to try fitting to the nature of the data aiming to have a model that could make an accurate prediction. Two parameters are provided as hyperparameters, strength (--C) and max number of iterations (--max_iter). The strength refers to the regularization of data that algorithm applies, the lower the value, the stronger becomes the regularization. Regularization is a technique used to reduce the "absorption of noise" by the model and avoiding overfitting, one of the main problems of machine learning. Max number of iterations sets the number of iterations taken for the selected algorithm to converge with the sought goal, 100% accuracy, in this case. Even though logistic regression technique is used, the problem to solve is a classification problem, with labeled data (supervised learning) that relates each sample with its class. We need to predict if a person would like to subscribe to a bank deposit or not. In a regression problem we would need to guess a number, not a class (e.g. houses cost in a region).

There are other algorithms to use such KNN Decision tree, Random Forrest or Gradient boosting that may fit better in other contexts if we follow the cheat-sheet provided by Scikit-Learn (https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html). To decide whether an algorithm works or not, we have to consider it's accuracy in relation with the nature of the problema we are trying to solve. Almost 92% of accuracy might be enough to say the chosen algorithm works for our problem. 

**What are the benefits of the parameter sampler you chose?**

**What are the benefits of the early stopping policy you chose?**

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
