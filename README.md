# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary

The dataset Bank Marketing (bankmarketing_train.csv) contains anonymized data over people (32,950) who suscribed to a fixed deposit, with variables such age, job title, marital status, education level, housing, loan, type of contact...and whether they finally suscribed it or not. With this data and a right prediction model we could be able to predict if a person could be a reasonable lead and become a customer of the offered service in a marketing campaign.

In this project, we are exploring two different ways of deploying an optimized Machine Learning (ML) pipeline in order to build that prediction model. A method in which a machine learning engineer must provide almost all the code except for hyperparameter tuning that could be accomplished using Azure HyperDrive package, and a method in which even this adjustment is done automatically along with the rest of the pipeline using Azure AutoML.

The best performing model with no meaningful differences against the custom model, has been the one generated by AutoML, with an accuracy of 91.55% Custom model has scored a bit lower with a 91.41% of accuracy. In the model generated by AutoML, the resulting algorithm has been Voting Ensemble that scored better than Logistic Regression model from SciKit-Learn, with a strength (--C) of 100 and 50 as max number of iterations (--max_iter).

## Common stages from both Pipelines
Both pipelines follow the classic ETL approach of Extracting data, Transforming data to "clean" data and Loading data into the machine learning algorithm, and in this exploration, both pipelines do use similar code to fulfill those stages.

1. Extracting data.

There is just one data source and it comes in a file with fields separated by commas, so providing it the file path a TabularDataset is created. As stated before, the dataset contains 32,950 rows and each row is related to a person and its different features captured during a Bank marketing campaign.

2. Transforming data.

Function to "clean" data makes use of Panda's library

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**


**What are the benefits of the parameter sampler you chose?**

**What are the benefits of the early stopping policy you chose?**

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
